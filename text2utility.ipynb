{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, requests, json, re\n",
    "import openai\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: From User Input we classify the Intent (Task) that is whether to book cab, order food, book movie tickets etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV and map labels\n",
    "df = pd.read_csv(\"synthetic_intent_dataset_v2.csv\")\n",
    "\n",
    "df.loc[df[\"label\"]==1, \"label\"] = 0\n",
    "df.loc[df[\"label\"]==2, \"label\"] = 1\n",
    "df.loc[df[\"label\"]==7, \"label\"] = 2\n",
    "df_in_context = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle with a fixed seed for reproducibility\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into Train and Test (70 : 30)\n",
    "train_size = int(0.7 * len(df))\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]\n",
    "\n",
    "# Save Train and Test Splits as CSV\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Splits Using `load_dataset`\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"})\n",
    "\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of labels\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Extract texts and labels from the dataset\n",
    "train_texts = [item[\"text\"] for item in dataset[\"train\"]]\n",
    "train_labels = [item[\"label\"] for item in dataset[\"train\"]]\n",
    "\n",
    "test_texts = [item[\"text\"] for item in dataset[\"test\"]]\n",
    "test_labels = [item[\"label\"] for item in dataset[\"test\"]]\n",
    "\n",
    "# Count label frequencies\n",
    "label_counter = Counter(train_labels)\n",
    "\n",
    "# Create a DataFrame from label frequencies\n",
    "df = pd.DataFrame.from_dict(label_counter, orient=\"index\", columns=[\"frequency\"])\n",
    "df.index.name = \"label\"\n",
    "df = df.sort_values(\"frequency\", ascending=False)\n",
    "\n",
    "# Plot the label frequencies\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "ax = df.plot(kind=\"bar\", rot=0, legend=False, title=\"Label Frequencies in Training Set\")\n",
    "ax.set_xlabel(\"Labels\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(train_texts,\n",
    "                                                                    train_labels,\n",
    "                                                                    test_size=0.1,\n",
    "                                                                    shuffle=True,\n",
    "                                                                    random_state=1)\n",
    "\n",
    "print(\"Train:\", len(train_texts))\n",
    "print(\"Dev:\", len(dev_texts))\n",
    "print(\"Test:\", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for Evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Finetuning with BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\"prajjwal1/bert-tiny\", \"prajjwal1/bert-small\", \"google-bert/bert-base-uncased\"]\n",
    "\n",
    "accuracies = []\n",
    "for model_id in model_ids:\n",
    "    print(f\"*** {model_id} ***\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    num_labels = len(set(train_labels))\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=num_labels)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(total_params)\n",
    "\n",
    "    # Convert lists back to PyTorch tensors for the model\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    dev_labels = torch.tensor(dev_labels)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "\n",
    "    print(f\"Train labels: {set(train_labels.tolist())}\")\n",
    "    print(f\"Dev labels: {set(dev_labels.tolist())}\")\n",
    "    print(f\"Test labels: {set(test_labels.tolist())}\")\n",
    "\n",
    "\n",
    "    train_texts_encoded = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    dev_texts_encoded = tokenizer(dev_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    test_texts_encoded = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    train_dataset = ClassificationDataset(train_texts_encoded, train_labels)\n",
    "    dev_dataset = ClassificationDataset(dev_texts_encoded, dev_labels)\n",
    "    test_dataset = ClassificationDataset(test_texts_encoded, test_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=int(len(train_dataset) / 16),\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        no_cuda=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "    trainer.save_model(f\"{os.getcwd()}/models/{model_id}\")\n",
    "\n",
    "    accuracies.append(test_results[\"eval_accuracy\"])\n",
    "    print(f\"Model {model_id} achieved accuracy: {test_results['eval_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training with PEFT\n",
    "model_ids = [\"prajjwal1/bert-tiny\", \"prajjwal1/bert-small\", \"google-bert/bert-base-uncased\"] #\"prajjwal1/bert-tiny\"\n",
    "\n",
    "accuracies = []\n",
    "for model_id in model_ids:\n",
    "    print(f\"*** {model_id} ***\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    num_labels = len(set(train_labels))\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=num_labels)\n",
    "\n",
    "    # **Step 1: Configure PEFT (LoRA)**\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "        inference_mode=False,       # Fine-tuning mode\n",
    "        r=64,                        # LoRA rank\n",
    "        lora_alpha=64,              # LoRA scaling factor\n",
    "        lora_dropout=0.2            # Dropout for LoRA layers\n",
    "    )\n",
    "\n",
    "    # **Step 2: Wrap the Base Model with PEFT**\n",
    "    model = get_peft_model(base_model, peft_config)\n",
    "    model.print_trainable_parameters()  # Print trainable parameters for verification\n",
    "\n",
    "    # Label mapping and preparation remain unchanged\n",
    "    train_labels = [int(label.item()) if isinstance(label, torch.Tensor) else int(label) for label in train_labels]\n",
    "    dev_labels = [int(label.item()) if isinstance(label, torch.Tensor) else int(label) for label in dev_labels]\n",
    "    test_labels = [int(label.item()) if isinstance(label, torch.Tensor) else int(label) for label in test_labels]\n",
    "\n",
    "    # label_mapping = {label: idx for idx, label in enumerate(sorted(set(train_labels)))}\n",
    "    # train_labels = [label_mapping[label] for label in train_labels]\n",
    "    # dev_labels = [label_mapping[label] for label in dev_labels]\n",
    "    # test_labels = [label_mapping[label] for label in test_labels]\n",
    "\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    dev_labels = torch.tensor(dev_labels)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "\n",
    "    train_texts_encoded = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    dev_texts_encoded = tokenizer(dev_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    test_texts_encoded = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    train_dataset = ClassificationDataset(train_texts_encoded, train_labels)\n",
    "    dev_dataset = ClassificationDataset(dev_texts_encoded, dev_labels)\n",
    "    test_dataset = ClassificationDataset(test_texts_encoded, test_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=int(len(train_dataset) / 16),\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        no_cuda=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "    )\n",
    "\n",
    "    # **Step 3: Train with PEFT**\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "    # Save model\n",
    "    trainer.save_model(f\"{os.getcwd()}/models/peft/{model_id}\")\n",
    "\n",
    "    accuracies.append(test_results[\"eval_accuracy\"])\n",
    "    print(f\"Model {model_id} achieved accuracy: {test_results['eval_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using the Fully Fine-Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python Function to help in the intent_classification based on a given prompt\n",
    "def classify_intent(query, tokenizer, model):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    print(logits)\n",
    "    print(logits.size())\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "def generate_prompt(verbalizer, input_sentence, intent_label) -> str:\n",
    "  return f\"{verbalizer}Input Sentence: {input_sentence}\\nIntent Label: {intent_label}\\nOutput:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"models/google-bert/bert-base-uncased\")\n",
    "\n",
    "query = \"How windy is it in Mumbai today?\"\n",
    "predicted_label = classify_intent(query, tokenizer, model)\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: From classified intent, we will generate JSON output for that toolâ€™s API call (Few shot prompting with json templates for api calls using GPT-4o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalizer = \"\"\"Based on the intent label (0, 1, 2), return the specific output as JSON. Below are examples:\n",
    "\n",
    "Example 1:\n",
    "Input Sentence: \"Get me a cab from CMU to UPitt for 3 people at 8.30pm.\"\n",
    "Intent Label: 0\n",
    "Output:\n",
    "{\n",
    "   \"from\": \"CMU\",               # Extract \"from\" location from the input sentence if given else return empty string \"\"\n",
    "   \"to\": \"UPitt\",               # Extract \"to\" location from the input sentence if given else return empty string \"\"\n",
    "   \"time\": \"8.30pm\",            # Extract time, if not found return empty string \"\"\n",
    "   \"number_of_people\": 3,       # Extract number of people, if not found return 1\n",
    "}\n",
    "\n",
    "Example 2:\n",
    "Input Sentence: \"Get me a cab to Target.\"\n",
    "Intent Label - 0\n",
    "Output:\n",
    "{\n",
    "  \"from\": \"\",\n",
    "  \"to\": \"Target\",\n",
    "  \"time\": \"\",\n",
    "  \"number_of_people\": 1\n",
    "}\n",
    "\n",
    "Example 3:\n",
    "Input Sentence: \"Will it rain tomorrow in London?\"\n",
    "Intent Label - 1\n",
    "Output:\n",
    "{\n",
    "  \"location\": \"London\",               # Extract location, if not found return \"current_location\"\n",
    "  \"forecast_days\": 2                  # Extract number of days, if not found return 1\n",
    "}\n",
    "\n",
    "Example 4:\n",
    "Input Sentence: \"How is the weather outside?\"\n",
    "Intent Label - 1\n",
    "Output:\n",
    "{\n",
    "  \"location\": \"current_location\",\n",
    "  \"forecast_days\": 1\n",
    "}\n",
    "\n",
    "Example 5:\n",
    "Input Sentence: \"Is it raining in New York?\"\n",
    "Intent Label - 1\n",
    "Output:\n",
    "{\n",
    "  \"location\": \"New York\",\n",
    "  \"forecast_days\": 1\n",
    "}\n",
    "\n",
    "Example 6:\n",
    "Input Sentence: \"Book a table for today's lunch at Eggy's Diner for 3 people\"\n",
    "Intent Label - 2\n",
    "Output:\n",
    "{\n",
    "  \"date\": today_date,          # Extract date like today's or tomorrow's date or any other day in the week's date, if not found return empty string \"\"\n",
    "  \"time\": \"\",                  # Extract time, if not found return empty string \"\"\n",
    "  \"type\": \"lunch\"              # Extract type, if not found return empty string \"\"\n",
    "  \"location\": \"Eggy's Diner\",  # Extract location, if not found return empty string \"\"\n",
    "  \"number_of_people\": 3        # Extract number of people, if not found return 1\n",
    "}\n",
    "\n",
    "Example 7:\n",
    "Input Sentence: \"Book a table at Joan's on Third for my family reunion on Saturday\"\n",
    "Intent Label - 2\n",
    "Output:\n",
    "{\n",
    "  \"date\": Saturday_date,\n",
    "  \"time\": \"\",\n",
    "  \"type\": \"family reunion\"\n",
    "  \"location\": \"Joan's on Third\",\n",
    "  \"number_of_people\": 1\n",
    "}\n",
    "\n",
    "\n",
    "Example 8:\n",
    "Input Sentence: \"Book a restaurant for 6 near my morning meeting tomorrow at 1pm\"\n",
    "Intent Label - 2\n",
    "Output:\n",
    "{\n",
    "  \"date\": tomorrow_date,\n",
    "  \"time\": \"1 pm\",\n",
    "  \"type\": \"\"\n",
    "  \"location\": \"near my morning meeting\",\n",
    "  \"number_of_people\": 6\n",
    "}\n",
    "\n",
    "\n",
    "Example 9:\n",
    "Input Sentence: \"Book a table at a restaurant near Times Square for 2 people tomorrow night\"\n",
    "Intent Label - 2\n",
    "Output:\n",
    "{\n",
    "  \"date\": tomorrow_date,\n",
    "  \"time\": \"night\",\n",
    "  \"type\": \"\"\n",
    "  \"location\": \"near Times Square\",\n",
    "  \"number_of_people\": 2\n",
    "}\n",
    "\n",
    "\n",
    "End of Examples.\n",
    "\n",
    "Now learning from the above examples, I want you to generate the output JSON for the given Input Sentence and Intent Label.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = generate_prompt(verbalizer, query, predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "  api_key = \"\",\n",
    "  base_url = \"\",\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that helps me generate JSON from a given prompt based on examples given!\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{input_prompt}\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_ = response.choices[0].message.content\n",
    "\n",
    "json_string = re.sub(r'json', '', json_).strip()\n",
    "json_string = re.sub(r'`|\\s+', ' ', json_string).strip()\n",
    "\n",
    "input_json = json.loads(json_string)\n",
    "# print(input_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating with Tools\n",
    "1. [Weather API](https://www.weatherapi.com/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_or_forecast(api_key, location, forecast_days=1):\n",
    "    if forecast_days > 1:\n",
    "        url = f\"https://api.weatherapi.com/v1/forecast.json?key={api_key}&q={location}&days={forecast_days}\"\n",
    "    else:\n",
    "        url = f\"https://api.weatherapi.com/v1/current.json?key={api_key}&q={location}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        weather_data = response.json()\n",
    "\n",
    "        if forecast_days > 1:\n",
    "            forecast = []\n",
    "            for day in weather_data[\"forecast\"][\"forecastday\"]:\n",
    "                forecast.append({\n",
    "                    \"date\": day[\"date\"],\n",
    "                    \"temperature_c\": {\n",
    "                        \"max\": day[\"day\"][\"maxtemp_c\"],\n",
    "                        \"min\": day[\"day\"][\"mintemp_c\"]\n",
    "                    },\n",
    "                    \"temperature_f\": {\n",
    "                        \"max\": day[\"day\"][\"maxtemp_f\"],\n",
    "                        \"min\": day[\"day\"][\"mintemp_f\"]\n",
    "                    },\n",
    "                    \"condition\": day[\"day\"][\"condition\"][\"text\"]\n",
    "                })\n",
    "            return {\n",
    "                \"location\": weather_data[\"location\"][\"name\"],\n",
    "                \"region\": weather_data[\"location\"][\"region\"],\n",
    "                \"country\": weather_data[\"location\"][\"country\"],\n",
    "                \"forecast_days\": forecast\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"temperature_c\": weather_data[\"current\"][\"temp_c\"],\n",
    "                \"temperature_f\": weather_data[\"current\"][\"temp_f\"],\n",
    "                \"condition\": weather_data[\"current\"][\"condition\"][\"text\"],\n",
    "                \"location\": weather_data[\"location\"][\"name\"],\n",
    "                \"region\": weather_data[\"location\"][\"region\"],\n",
    "                \"country\": weather_data[\"location\"][\"country\"]\n",
    "            }\n",
    "    else:\n",
    "        print(\"Error fetching weather/forecast:\", response.text)\n",
    "        return None\n",
    "    \n",
    "def get_current_location():\n",
    "    url = \"https://ipinfo.io/json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        location = data.get(\"city\", \"unknown\")\n",
    "        return location\n",
    "    else:\n",
    "        print(\"Error fetching location:\", response.text)\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_API_KEY = \"Put Your API key\"\n",
    "\n",
    "location = None\n",
    "if input_json[\"location\"] == \"current_location\":\n",
    "  location = get_current_location()\n",
    "  input_json[\"location\"] = location\n",
    "else:\n",
    "  location = input_json[\"location\"]\n",
    "\n",
    "print(location)\n",
    "\n",
    "dt = datetime.now()\n",
    "eastern = timezone('US/Eastern')\n",
    "current_time = dt.astimezone(eastern)\n",
    "input_json[\"time\"] = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(input_json)\n",
    "\n",
    "result = get_weather_or_forecast(WEATHER_API_KEY, input_json[\"location\"], input_json[\"forecast_days\"])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmu-llms-hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
